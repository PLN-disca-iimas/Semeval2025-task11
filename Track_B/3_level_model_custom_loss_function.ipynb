{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Bwg_lb9bCFa"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers torch scikit-learn unidecode datasets"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Importar las librerías\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, AutoTokenizer,  AutoModelForSequenceClassification\n"
   ],
   "metadata": {
    "id": "C-9CNuucbmF-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Detectar dispositivo (GPU o CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "6hQejWAaWwS7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device"
   ],
   "metadata": {
    "id": "-aX-vLROWw2R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lang = \"ukr\"\n",
    "emotion = \"surprise\"\n",
    "model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "max_length = 71\n",
    "num_epochs = 3\n",
    "adam_lr=2e-5\n",
    "\n",
    "save_model = True\n",
    "\n",
    "# Leer los archivos de Google Drive\n",
    "train_path = f'/content/drive/MyDrive/Proyectos/semeval/data/newest/train/{lang}.csv'\n",
    "val_path = f'/content/drive/MyDrive/Proyectos/semeval/data/newest/dev/{lang}.csv'\n",
    "test_path = f'/content/drive/MyDrive/Proyectos/semeval/data/newest/dev/{lang}.csv'\n",
    "\n",
    "output_dir = f'/content/drive/MyDrive/Proyectos/semeval/models/{lang}/{emotion}_3_level/'"
   ],
   "metadata": {
    "id": "in2DAc9nbmpY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_test = pd.read_csv(test_path)"
   ],
   "metadata": {
    "id": "HGl6zIwaUaGv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train = pd.concat([df_train, df_val], ignore_index=True)"
   ],
   "metadata": {
    "id": "NnVICcXZcx5R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train = df_train[[\"text\", emotion]]\n",
    "df_val = df_val[[\"text\", emotion]]\n",
    "df_test = df_test[[\"text\", emotion]]\n",
    "\n",
    "df_train = df_train[df_train[emotion] != 0]\n",
    "df_val = df_val[df_val[emotion] != 0]\n",
    "df_test = df_test[df_test[emotion] != 0]"
   ],
   "metadata": {
    "id": "Sko-5zESRwhG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train.columns = [\"text\", \"label\"]\n",
    "df_val.columns = [\"text\", \"label\"]\n",
    "df_test.columns = [\"text\", \"label\"]"
   ],
   "metadata": {
    "id": "vNpCuoG6d2Jn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train"
   ],
   "metadata": {
    "id": "-upZ8B0PePFN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Mapeo de etiquetas a índices\n",
    "label_mapping = {1:0,  2:1, 3:2}\n",
    "df_train['label'] = df_train['label'].map(label_mapping)\n",
    "df_val['label'] = df_val['label'].map(label_mapping)\n",
    "df_test['label'] = df_test['label'].map(label_mapping)\n"
   ],
   "metadata": {
    "id": "FWtwKw6Ed9hV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convertir a Dataset de Hugging Face\n",
    "train_dataset = Dataset.from_pandas(df_train[['text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(df_val[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(df_test[['text', 'label']])\n"
   ],
   "metadata": {
    "id": "vJM_o8KJeQR6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "id": "aBGN_MZneVx4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenización\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n"
   ],
   "metadata": {
    "id": "wIw2HOjPeS-4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ],
   "metadata": {
    "id": "J1fsmiJ3f1st"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Definir el modelo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.config.hidden_dropout_prob = 0.3  # Ajustar el dropout al 30%\n"
   ],
   "metadata": {
    "id": "FcKRm60-gDqH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Mover el modelo al dispositivo\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "id": "f-oerntjW7hj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='sum'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(logits, labels)\n",
    "        pt = torch.exp(-ce_loss)  # Probabilidades predichas para la clase verdadera\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss  # Sin reducción"
   ],
   "metadata": {
    "id": "YdI7t3aYUeJh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, class_weights, focal_alpha=0.25, focal_gamma=2.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        # Weighted Cross-Entropy Loss\n",
    "        self.weighted_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        # Weighted Smooth Cross-Entropy Loss\n",
    "        self.smooth_loss = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "        # Focal Loss personalizada\n",
    "        self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Calcular pérdidas\n",
    "        focal_loss = self.focal_loss(logits, labels)\n",
    "        weighted_loss = self.weighted_loss(logits, labels)\n",
    "        smooth_loss = self.smooth_loss(logits, labels)\n",
    "\n",
    "        # Promediar las pérdidas\n",
    "        return (focal_loss + weighted_loss + smooth_loss) / 3"
   ],
   "metadata": {
    "id": "R_FD25J0QQKF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Definir métrica de evaluación\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n"
   ],
   "metadata": {
    "id": "Mo64K4FRgI-d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Argumentos del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",  # Evaluar en cada epoch usando el conjunto de validación\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ],
   "metadata": {
    "id": "A22Cy-dXgNJk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Pesar las clases\n",
    "class_counts = df_train['label'].value_counts()\n",
    "class_weights = torch.tensor([1.0 / class_counts[i] for i in range(len(class_counts))], dtype=torch.float32).to(device)\n"
   ],
   "metadata": {
    "id": "QFelcK4qVAl6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss_fn = CustomLoss(class_weights=class_weights)"
   ],
   "metadata": {
    "id": "Ut-0D7edVOO1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").to(model.device)  # Mover etiquetas al mismo dispositivo que el modelo\n",
    "        outputs = model(**inputs)  # Obtener las salidas del modelo\n",
    "        logits = outputs.logits  # Extraer logits\n",
    "        loss = loss_fn(logits, labels)  # Calcular la pérdida personalizada\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "metadata": {
    "id": "mCKpHCGLVQJZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Entrenador\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # Usamos el conjunto de validación para evaluar en cada epoch\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(AdamW(model.parameters(), lr=adam_lr), None)\n",
    "    #optimizers=(SGD(model.parameters(), lr=0.01, momentum=0.9), None)\n",
    "\n",
    ")\n"
   ],
   "metadata": {
    "id": "_6eVMMKsgTBV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Entrenar el modelo\n",
    "trainer.train()"
   ],
   "metadata": {
    "id": "vtLwJNg7gYM-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tjM6SpwZWOWb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JDyGUvRlWOb0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# guardado y evaluacion"
   ],
   "metadata": {
    "id": "EfLgm6HXWPBk"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FO_sPQQxWQq_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()"
   ],
   "metadata": {
    "id": "4vF-Cx8elC7Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Función collate para convertir a tensores\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Crear un DataLoader que use la función collate\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "id": "nDWRSkEoWk7E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Realizar inferencias sobre el conjunto de test\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    # Enviar los tensores a la GPU o CPU\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.device)\n",
    "    labels = batch['labels'].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n"
   ],
   "metadata": {
    "id": "-X-vtYr4Xpj7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "t1 = len(list(set(all_labels)))\n",
    "t2 = len(list(set(all_preds)))\n",
    "\n",
    "t3 = max(t1,t2)\n",
    "target_names = [str(i+1) for i in range(0,t3)]"
   ],
   "metadata": {
    "id": "zV9BhJItdWGe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calcular las métricas por clase y globales\n",
    "report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "print(report)\n"
   ],
   "metadata": {
    "id": "1QQ73ctRXsV5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular y dibujar la matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['1', '2', '3'], yticklabels=['1', '2', '3'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "gZ4ATiJbYRTN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Guardar el modelo en Google Drive\n",
    "\n",
    "if save_model:\n",
    "  model.save_pretrained(output_dir)\n",
    "  tokenizer.save_pretrained(output_dir)\n"
   ],
   "metadata": {
    "id": "VnwXnjqxUHtG"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
